---
layout: page
title: "W4 - Transformer Network"
permalink: /c5-recurrent-neural-networks/week4/
parent: "C5 - Sequence Models"
---
# Transformer Network

Learning Objectives
- Create positional encodings to capture sequential relationships in data
- Calculate scaled dot-product self-attention with word embeddings
- Implement masked multi-head attention
- Build and train a Transformer model
- Fine-tune a pre-trained transformer model for Named Entity Recognition
- Fine-tune a pre-trained transformer model for Question Answering
- Implement a QA model in TensorFlow and PyTorch
- Fine-tune a pre-trained transformer model to a custom dataset
- Perform extractive Question Answering

# Transformers

## Transformer Network Intuition

> <img src="./images/w04-01-Transformer_Network_Intuition/img_2023-05-15_14-50-29.png">
> <img src="./images/w04-01-Transformer_Network_Intuition/img_2023-05-15_14-50-32.png">

## Self-Attention

> <img src="./images/w04-02-Self-Attention/img_2023-05-15_14-50-55.png">
> <img src="./images/w04-02-Self-Attention/img_2023-05-15_14-50-57.png">

## Multi-Head Attention

> <img src="./images/w04-03-Multi-Head_Attention/img_2023-05-15_14-51-07.png">

## Transformer Network

> <img src="./images/w04-04-Transformer_Network/img_2023-05-15_14-51-16.png">

# Conclusion

## Conclusion and Thank You!

> <img src="./images/w04-05-Conclusion_and_Thank_You/img_2023-05-15_14-54-08.png">
> <img src="./images/w04-05-Conclusion_and_Thank_You/img_2023-05-15_14-54-27.png">

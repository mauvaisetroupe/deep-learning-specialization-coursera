# Natural Language Processing & Word Embeddings

Natural language processing with deep learning is a powerful combination. Using word vector representations and embedding layers, train recurrent neural networks with outstanding performance across a wide variety of applications, including sentiment analysis, named entity recognition and neural machine translation.

Learning Objectives
- Explain how word embeddings capture relationships between words
- Load pre-trained word vectors
- Measure similarity between word vectors using cosine similarity
- Use word embeddings to solve word analogy problems such as Man is to Woman as King is to __.
- Reduce bias in word embeddings
- Create an embedding layer in Keras with pre-trained word vectors
- Describe how negative sampling learns word vectors more efficiently than other methods
- Explain the advantages and disadvantages of the GloVe algorithm
- Build a sentiment classifier using word embeddings
- Build and train a more sophisticated classifier using an LSTM

# Introduction to Word Embeddings

## Word Representation

## Using Word Embeddings

## Properties of Word Embeddings

## Embedding Matrix

# Learning Word Embeddings: Word2vec &amp; GloVe

## Learning Word Embeddings

## Word2Vec

## Negative Sampling

## GloVe Word Vectors

# Applications Using Word Embeddings

## Sentiment Classification

## Debiasing Word Embeddings

# Natural Language Processing & Word Embeddings

Natural language processing with deep learning is a powerful combination. Using word vector representations and embedding layers, train recurrent neural networks with outstanding performance across a wide variety of applications, including sentiment analysis, named entity recognition and neural machine translation.

Learning Objectives
- Explain how word embeddings capture relationships between words
- Load pre-trained word vectors
- Measure similarity between word vectors using cosine similarity
- Use word embeddings to solve word analogy problems such as Man is to Woman as King is to __.
- Reduce bias in word embeddings
- Create an embedding layer in Keras with pre-trained word vectors
- Describe how negative sampling learns word vectors more efficiently than other methods
- Explain the advantages and disadvantages of the GloVe algorithm
- Build a sentiment classifier using word embeddings
- Build and train a more sophisticated classifier using an LSTM

# Introduction to Word Embeddings

## Word Representation

## Using Word Embeddings

## Properties of Word Embeddings

## Embedding Matrix

# Learning Word Embeddings: Word2vec &amp; GloVe

## Learning Word Embeddings

## Word2Vec

## Negative Sampling

## GloVe Word Vectors

# Applications Using Word Embeddings

## Sentiment Classification

## Debiasing Word Embeddings


> <img src="./images/w02-02-word_representation/img_2023-05-02_07-56-25.png">
> <img src="./images/w02-02-word_representation/img_2023-05-02_07-56-28.png">
> <img src="./images/w02-02-word_representation/img_2023-05-02_07-56-30.png">
> <img src="./images/w02-03-using_word_embeddings/img_2023-05-02_07-58-47.png">
> <img src="./images/w02-03-using_word_embeddings/img_2023-05-02_07-58-49.png">
> <img src="./images/w02-03-using_word_embeddings/img_2023-05-02_07-58-52.png">
> <img src="./images/w02-04-properties_of_word_embeddings/img_2023-05-02_08-00-17.png">
> <img src="./images/w02-04-properties_of_word_embeddings/img_2023-05-02_08-00-19.png">
> <img src="./images/w02-04-properties_of_word_embeddings/img_2023-05-02_08-00-21.png">
> <img src="./images/w02-05-embedding_matrix/img_2023-05-02_08-00-33.png">
> <img src="./images/w02-07-learning_word_embeddings/img_2023-05-02_08-00-50.png">
> <img src="./images/w02-07-learning_word_embeddings/img_2023-05-02_08-00-51.png">
> <img src="./images/w02-08-word2vec/img_2023-05-02_08-01-03.png">
> <img src="./images/w02-08-word2vec/img_2023-05-02_08-01-05.png">
> <img src="./images/w02-08-word2vec/img_2023-05-02_08-01-07.png">
> <img src="./images/w02-09-negative_sampling/img_2023-05-02_08-01-19.png">
> <img src="./images/w02-09-negative_sampling/img_2023-05-02_08-01-21.png">
> <img src="./images/w02-09-negative_sampling/img_2023-05-02_08-01-23.png">
> <img src="./images/w02-10-glove_word_vectors/img_2023-05-02_08-01-33.png">
> <img src="./images/w02-10-glove_word_vectors/img_2023-05-02_08-01-35.png">
> <img src="./images/w02-10-glove_word_vectors/img_2023-05-02_08-01-37.png">
> <img src="./images/w02-12-sentiment_classification/img_2023-05-02_08-09-41.png">
> <img src="./images/w02-12-sentiment_classification/img_2023-05-02_08-09-43.png">
> <img src="./images/w02-12-sentiment_classification/img_2023-05-02_08-09-45.png">
> <img src="./images/w02-13-debiasing_word_embeddings/img_2023-05-02_08-10-01.png">
> <img src="./images/w02-13-debiasing_word_embeddings/img_2023-05-02_08-10-03.png">


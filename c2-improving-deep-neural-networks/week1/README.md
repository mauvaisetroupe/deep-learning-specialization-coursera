# Practical Aspects of Deep Learning

Discover and experiment with a variety of different initialization methods, apply L2 regularization and dropout to avoid model overfitting, then apply gradient checking to identify errors in a fraud detection model.

Learning Objectives
- Give examples of how different types of initializations can lead to different results
- Examine the importance of initialization in complex neural networks
- Explain the difference between train/dev/test sets
- Diagnose the bias and variance issues in your model
- Assess the right time and place for using regularization methods such as dropout or L2 regularization
- Explain Vanishing and Exploding gradients and how to deal with them
- Use gradient checking to verify the accuracy of your backpropagation implementation
- Apply zeros initialization, random initialization, and He initialization
- Apply regularization to a deep learning model


# Setting up your Machine Learning Application

##  Train / Dev / Test sets

##  Bias / Variance

##  Basic Recipe for Machine Learning



# Regularizing your Neural Network

##  Regularization

##  Why Regularization Reduces Overfitting?

##  Dropout Regularization

##  Understanding Dropout

##  Other Regularization Methods



# Setting Up your Optimization Problem

##  Normalizing Inputs

##  Vanishing / Exploding Gradients

##  Weight Initialization for Deep Networks

##  Numerical Approximation of Gradients

##  Gradient Checking

##  Gradient Checking Implementation Notes


# Heroes of deep learning

##  Yoshua Bengio Interview